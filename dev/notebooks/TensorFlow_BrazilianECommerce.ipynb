{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Project-Variables\" data-toc-modified-id=\"Project-Variables-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Project Variables</a></span></li><li><span><a href=\"#Dataset-and-Pipelines\" data-toc-modified-id=\"Dataset-and-Pipelines-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Dataset and Pipelines</a></span></li><li><span><a href=\"#Building-the-Deep-NN\" data-toc-modified-id=\"Building-the-Deep-NN-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Building the Deep NN</a></span><ul class=\"toc-item\"><li><span><a href=\"#Network-Variables\" data-toc-modified-id=\"Network-Variables-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Network Variables</a></span></li><li><span><a href=\"#Construction-Phase\" data-toc-modified-id=\"Construction-Phase-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Construction Phase</a></span></li><li><span><a href=\"#Execution-Phase\" data-toc-modified-id=\"Execution-Phase-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Execution Phase</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is building a sentiment classification model using plain TensorFlow. For making this task easier, we will use data prep steps (like pipelines and text functions) for preparing the data for feeding a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T23:27:08.302688Z",
     "start_time": "2020-09-28T23:27:05.455820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\thipa\\Anaconda3\\lib\\site-packages\\utils\\dnn_utils.py:830: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from joblib import load\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Utilities\n",
    "from utils.custom_transformers import import_data\n",
    "from utils.dnn_utils import fetch_batch\n",
    "from utils.viz_utils import format_spines\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T23:27:08.318143Z",
     "start_time": "2020-09-28T23:27:08.305683Z"
    }
   },
   "outputs": [],
   "source": [
    "# Variables for path definition\n",
    "DATA_PATH = '../../data'\n",
    "PIPE_PATH = '../../pipelines'\n",
    "\n",
    "# Variables reading files\n",
    "DATASET_FILENAME = 'olist_order_reviews_dataset.csv'\n",
    "DATASET_COLS = ['review_comment_message', 'review_score']\n",
    "FEATURES_COL = 'review_comment_message'\n",
    "TARGET_COL = 'review_score'\n",
    "\n",
    "# Variables for reading pipelines\n",
    "TEXT_PIPE = 'text_prep_pipeline.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now, let's read the raw data and apply the text prep pipeline already built on python training script on `dev/training` project folder. The goal is to give the raw text as input and transform this data into features using the vectorizer implemented on the pipeline (`TfIdfVectorizer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T23:27:42.260503Z",
     "start_time": "2020-09-28T23:27:08.321053Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Shape of X_train data: (33402, 650)\n",
      "--------------------------------------------------\n",
      "Shape of X_test data: (8351, 650)\n",
      "\n",
      "Samples of y_train: [1 4 4 5 5]\n",
      "Samples of y_test: [1 1 3 3 1]\n"
     ]
    }
   ],
   "source": [
    "# Reading the data and dropping duplicates\n",
    "df = pd.read_csv(os.path.join(DATA_PATH, DATASET_FILENAME), usecols=DATASET_COLS)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Splitting the data into train and test\n",
    "X = df[FEATURES_COL].values\n",
    "y = df[TARGET_COL].values\n",
    "\n",
    "# Reading the pipeline\n",
    "text_prep_pipe = load(os.path.join(PIPE_PATH, TEXT_PIPE))\n",
    "\n",
    "# Applying it to training data\n",
    "X_prep = text_prep_pipe.fit_transform(X)\n",
    "\n",
    "# Splitting into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_prep, y, test_size=.20, random_state=42)\n",
    "\n",
    "# Converting to array\n",
    "X_train = X_train.toarray()\n",
    "X_test = X_test.toarray()\n",
    "\n",
    "# Results\n",
    "for data, name in zip([X_train, X_test], ['X_train', 'X_test']):\n",
    "    print('-' * 50)\n",
    "    print(f'Shape of {name} data: {data.shape}')\n",
    "print(f'\\nSamples of y_train: {y_train[:5]}')\n",
    "print(f'Samples of y_test: {y_test[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Deep NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After rading and preparing the data for feeding it into a neural network, let's retrieve some useful parameters for the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T01:20:17.623797Z",
     "start_time": "2020-09-29T01:20:17.597866Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Neural network inputs: 650\n",
      "Number of classes: 5 - Sample: [1 0 0 0 0]\n",
      "\n",
      "Structure:\n",
      "650 -> 300 -> 100 -> 50 -> 25 -> 25 -> 5\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Retrieving data info\n",
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = len(np.unique(y_train))\n",
    "\n",
    "# Transforming the classes in one hot vectors\n",
    "y_train_oh = pd.get_dummies(y_train).values.astype('int')\n",
    "y_test_oh = pd.get_dummies(y_test).values.astype('int')\n",
    "\n",
    "# Neural network structure\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 25\n",
    "n_hidden5 = 25\n",
    "\n",
    "# Overview\n",
    "print('-' * 40)\n",
    "print(f'Neural network inputs: {n_inputs}')\n",
    "print(f'Number of classes: {n_outputs} - Sample: {y_train_oh[0]}')\n",
    "print(f'\\nStructure:')\n",
    "print(f'{n_inputs} -> {n_hidden1} -> {n_hidden2} -> {n_hidden3} -> {n_hidden4} -> {n_hidden5} -> {n_outputs}')\n",
    "print('-' * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build the computation graph for our Neural Network using TensorFlow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T23:36:15.538653Z",
     "start_time": "2020-09-28T23:36:13.983606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A25A5D080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A25A5D080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A25A5D080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A25A5D080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A256F46A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A256F46A0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A256F46A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A256F46A0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A256F46A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A256F46A0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A256F46A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A256F46A0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A256F46A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A256F46A0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A256F46A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A256F46A0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A256F46A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A256F46A0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A256F46A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A256F46A0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A256F46A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A256F46A0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A256F46A0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x0000020A256F46A0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# ---- CONSTRUCTION PHASE ----\n",
    "# ----------------------------\n",
    "\n",
    "# Creating placeholders for receiving mini-batches\n",
    "tf.reset_default_graph()\n",
    "with tf.name_scope('inputs'):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "    y = tf.placeholder(tf.int32, shape=(None, n_outputs), name='y')\n",
    "    \n",
    "# Building layers\n",
    "with tf.name_scope('layers'):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name='hidden1')\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name='hidden2')\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name='hidden3')\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name='hidden4')\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name='hidden5')\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name='outputs')\n",
    "    \n",
    "# Defining cost function\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name='loss')\n",
    "    \n",
    "# Defining optimizer\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "# Node for performance evaluation\n",
    "with tf.name_scope('accuracy'):\n",
    "    pred = tf.argmax(logits, 1)\n",
    "    correct = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name='accuracy')\n",
    "    \n",
    "# Node for initializing all global variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T01:20:17.169969Z",
     "start_time": "2020-09-29T01:18:10.022234Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:     0 - Acc Train: 0.7266 - Acc Test: 0.6808\n",
      "Epoch:     5 - Acc Train: 0.8672 - Acc Test: 0.663\n",
      "Epoch:    10 - Acc Train: 0.875 - Acc Test: 0.6387\n",
      "Epoch:    15 - Acc Train: 0.9062 - Acc Test: 0.6554\n",
      "Epoch:    20 - Acc Train: 0.9531 - Acc Test: 0.6594\n",
      "Epoch:    25 - Acc Train: 0.9219 - Acc Test: 0.6331\n",
      "Epoch:    30 - Acc Train: 0.9062 - Acc Test: 0.6372\n",
      "Epoch:    35 - Acc Train: 0.875 - Acc Test: 0.6344\n",
      "Epoch:    40 - Acc Train: 0.9297 - Acc Test: 0.6373\n",
      "Epoch:    45 - Acc Train: 0.9219 - Acc Test: 0.636\n",
      "Epoch:    50 - Acc Train: 0.8672 - Acc Test: 0.627\n",
      "Epoch:    55 - Acc Train: 0.9141 - Acc Test: 0.6296\n",
      "Epoch:    60 - Acc Train: 0.8984 - Acc Test: 0.638\n",
      "Epoch:    65 - Acc Train: 0.9453 - Acc Test: 0.6454\n",
      "Epoch:    70 - Acc Train: 0.9297 - Acc Test: 0.6393\n",
      "Epoch:    75 - Acc Train: 0.9141 - Acc Test: 0.6344\n",
      "Epoch:    80 - Acc Train: 0.8672 - Acc Test: 0.6509\n",
      "Epoch:    85 - Acc Train: 0.9062 - Acc Test: 0.6338\n",
      "Epoch:    90 - Acc Train: 0.8906 - Acc Test: 0.6342\n",
      "Epoch:    95 - Acc Train: 0.9297 - Acc Test:  0.63\n"
     ]
    }
   ],
   "source": [
    "# Defining training variables\n",
    "m_train = X_train.shape[0]\n",
    "n_epochs = 100\n",
    "batch_size = 128\n",
    "n_batches = m_train // batch_size\n",
    "costs = []\n",
    "\n",
    "# Initializing a TensorFlow session\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initializing global variables\n",
    "    init.run()\n",
    "    \n",
    "    # Iterating over training epochs\n",
    "    for epoch in range(n_epochs):\n",
    "        # Iterating over mini-batches\n",
    "        for batch in range(n_batches):\n",
    "            X_batch, y_batch = fetch_batch(X_train, y_train_oh, epoch, batch, batch_size)\n",
    "            \n",
    "            # Running training using mini-batches\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            \n",
    "        # Evaluating metrics each 5 epochs\n",
    "        if epoch % 5 == 0:\n",
    "            acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test_oh})\n",
    "            print(f'Epoch: {epoch:>5} - Acc Train: {round(float(acc_train), 4):>5} - Acc Test: {round(float(acc_test), 4):>5}')\n",
    "        \n",
    "        # Evaluating costs\n",
    "        cost = loss.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        costs.append(cost)\n",
    "        \n",
    "    # Storing logits and predictions after training through all epochs\n",
    "    train_logits = logits.eval(feed_dict={X: X_train, y: y_train_oh})\n",
    "    train_pred = pred.eval(feed_dict={X: X_train, y: y_train_oh}) + 1 # + 1 para alinhar score = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T01:20:26.973673Z",
     "start_time": "2020-09-29T01:20:26.953721Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.365871  ,  2.728673  ,  0.82380456, -2.8690126 , -5.2494516 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T01:20:33.398035Z",
     "start_time": "2020-09-29T01:20:33.389048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-29T01:20:41.810854Z",
     "start_time": "2020-09-29T01:20:41.801878Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T23:34:07.464611Z",
     "start_time": "2020-09-28T23:34:07.458076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-17.013744  -10.668154  -13.117463   13.62727    -1.0321145]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(train_logits[2])\n",
    "print(train_pred[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T23:34:32.710650Z",
     "start_time": "2020-09-28T23:34:32.703480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T23:29:42.402424Z",
     "start_time": "2020-09-28T23:27:05.499Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotando custo\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(np.squeeze(costs), color='navy')\n",
    "format_spines(ax, right_border=False)\n",
    "ax.set_title('Neural Network Cost', color='dimgrey')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Cost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "214.273px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
